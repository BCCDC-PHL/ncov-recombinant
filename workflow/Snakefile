"""
@author: Katherine Eaton
SARS-CoV-2 Recombinant Detection Pipeline.
"""

# -----------------------------------------------------------------------------#
#                             Modules and Packages                             #
# -----------------------------------------------------------------------------#
import os
import copy
import textwrap
import datetime
import subprocess
import pandas as pd
import json

# Enforce minimum version
from snakemake.utils import min_version
min_version("7.3.6")

# -----------------------------------------------------------------------------#
# Setup
# -----------------------------------------------------------------------------#

today = datetime.date.today()

# Begin Code: https://github.com/nextstrain/ncov/blob/master/Snakefile

# Store the user's configuration prior to loading defaults. We need to make a deep
# copy because Snakemake will deep merge the dictionary later, modifying the values
# of a reference or shallow copy. Note that this loading of
# the user's config prior to the defaults below depends on the order Snakemake
# loads its configfiles. Specifically, the order of config loading is:
#
# 1. First, configfile arguments are loaded and config is built from these [1].
# 2. Then, config arguments are loaded and override existing config keys [2].
# 3. Then, the Snakefile is parsed and configfile directive inside the Snakefile is processed [3].
#    When configfile is loaded from the directive in the Snakefile, the config
#    dictionary is deep merged with the files [4] from the externally provided
#    config files. This is the only place the deep merge happens using the
#    update_config function [5].

# Load the default parameters
configfile: "defaults/parameters.yaml"

# Convert config into an OrderedDict with keys of "name" for use by the pipeline.
if isinstance(config.get("inputs"), list):
    config["inputs"] = OrderedDict((v["name"], v) for v in config["inputs"])
if isinstance(config.get("builds"), list):
    config["builds"] = OrderedDict((v["name"], v) for v in config["builds"])
if isinstance(config.get("rule_params"), list):
    config["rule_params"] = OrderedDict((v["name"], v) for v in config["rule_params"])

# Check for missing inputs.
if "inputs" not in config:
    logger.error("ERROR: Your workflow does not define any input files to start with.")
    logger.error("Update your configuration file (e.g., 'builds.yaml') to define at least one input dataset as follows and try running the workflow again:")
    logger.error(textwrap.indent(
        f"\ninputs:\n  name: local-data\n  metadata: data/example_metadata.tsv\n  sequences: data/example_sequences.fasta.gz\n",
        "  "
    ))
    sys.exit(1)

# Check for missing builds
if "builds" not in config:
    logger.error("ERROR: Your workflow does not define any builds to start with.")
    logger.error("Update your configuration file (e.g., 'builds.yaml') to define at least one build as follows and try running the workflow again:")
    logger.error(textwrap.indent(
        f"\nbuilds:\n  name: example-build\n",
        "  "
    ))
    sys.exit(1)

# End Code: https://github.com/nextstrain/ncov/blob/master/Snakefile

default_keys = ["inputs", "builds", "rule_params"]
default_build_keys = ["name", "base_input"]

# Set default rule parameters for each build
for build in config["builds"]:
  for rule in config["rule_params"]:
    # Case 1. There are no params for this rule in the build
    if rule not in config["builds"][build]:
      config["builds"][build][rule] = config["rule_params"][rule]
    # Case 2. There are some params for this rule in the build
    else:
      for param in config["rule_params"][rule]:
        # Case 3. This param has not been specified
        if param not in config["builds"][build][rule]:
          config["builds"][build][rule][param] = config["rule_params"][rule][param]

# Store list of builds and inputs for rule wildcard constraints
BUILDS = list(config["builds"].keys())
USHER_INPUTS = [input for input in config["inputs"] if "usher" in config["inputs"][input]["type"]]
LOCAL_INPUTS = [input for input in config["inputs"] if "local" in config["inputs"][input]["type"]]

# Check if any builds are controls for validation
validation_df = pd.read_csv("defaults/validation.tsv", sep="\t")
BUILDS_VALIDATE = []
for build in BUILDS:
  if build in list(validation_df["build"]):
    BUILDS_VALIDATE.append(build)

# -----------------------------------------------------------------------------#
#  Default Target
# -----------------------------------------------------------------------------#

rule all:
  """
  Default workflow targets.
  """
    input:
        # Config files
        expand("results/{build_name}/config.json",
          build_name=BUILDS,
          ),
        # Stage 1: Nextclade
        expand("results/{build_name}/nextclade/alignment.fasta",
          build_name=BUILDS,
          ),
        # Stage 2: sc2rf
        expand("results/{build_name}/sc2rf/stats.tsv",
          build_name=BUILDS,
          ),
        # Stage 3: UShER
        expand("results/{build_name}/usher/usher.clades.tsv",
          build_name=BUILDS,
          ),
        # Stage 4: Reports
        expand("results/{build_name}/report/report.xlsx",
          build_name=BUILDS,
          ),
        expand("results/{build_name}/report_historical/report.pptx",
                  build_name=BUILDS,
                  ),
       # Stage 5: Validation (optional)
        expand("results/{build_name}/validate/validation.tsv",
          build_name=BUILDS_VALIDATE,
          ),

# -----------------------------------------------------------------------------#
#  Accessory Rules
# -----------------------------------------------------------------------------#

rule print_config:
  """
  Print full config.
  """
  message: "Printing full config."
  run:
    # Print the config
    config_json = json.dumps(config, indent = 2)
    print(config_json)


# -----------------------------------------------------------------------------#
rule save_config:
  """
  Save full config.
  """
  message: "Saving full config."
  output:
    json = "results/{build}/config.json",
  params:
    output_dir = "results/{build}/",
  run:
    config_copy = copy.deepcopy(config)
    # Remove other builds from config
    for build in config["builds"]:
      if build != wildcards.build:
        del config_copy["builds"][build]

    config_json = json.dumps(config_copy, indent = 2)

    # Create output directory
    if not os.path.exists(params.output_dir):
      os.mkdir(params.output_dir)

    # Save to json file
    outfile_path = output.json
    with open(outfile_path, "w") as outfile:
      outfile.write(config_json)

# -----------------------------------------------------------------------------#
rule help:
  """
  Print list of all rules and targets with help.
  """
  run:
    for rule in workflow.rules:
      print("-" * 160)
      print("rule: ", rule.name )
      if rule.docstring:
          print(rule.docstring)
      if rule._input:
          print("\tinput:")
          for in_file in rule.input:
              print("\t\t" + str(in_file))
          for in_file in rule.input.keys():
              print("\t\t" + in_file + ": " + str(rule.input[in_file]))
      if rule._output:
          print("\toutput:")
          for out_file in rule.output:
              print("\t\t" + out_file)
          for out_file in rule.output.keys():
              print("\t\t" + out_file + ": " + str(rule.output[out_file]))
      if rule._params:
          print("\tparams:")
          for param in rule.params.keys():
              print("\t\t" + param + ": " + str(rule.params[param]))
      if rule.resources:
          print("\tresources:")
          for resource in rule.resources.keys():
              print("\t\t" + resource + ": " + str(rule.resources[resource]))
      if rule.conda_env:
          print("\t\tconda: ", rule.conda_env)
      if rule._log:
          print("\t\tlog: ", rule._log)

# -----------------------------------------------------------------------------#
#  STAGE 0: Setup
# -----------------------------------------------------------------------------#

rule_name = "issues_download"
rule issues_download:
  """Download pango-designation issues."""

  message: """Downloading pango-designation issues.\n
  log:     {log}
  issues:  {output.issues}
  plot:    {output.breakpoints_png}
  """

  input:
    breakpoints      = "resources/breakpoints.tsv",
  output:
    issues           = "resources/issues.tsv",
    issue_to_lineage = "resources/issue_to_lineage.tsv",
    breakpoints_png  = "resources/breakpoints_clade.png",
    breakpoints_svg  = "resources/breakpoints_clade.svg",
    breakpoints_tsv  = "resources/breakpoints_clade.tsv",
  params:
    outdir = "resources",
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 scripts/download_issues.py --breakpoints {input.breakpoints} 1> {output.issues} 2> {log};
    csvtk cut -t -f "issue,lineage" {output.issues} | tail -n+2   1> {output.issue_to_lineage} 2>> {log};
    python3 scripts/plot_breakpoints.py --lineages {input.breakpoints} --outdir {params.outdir} --autoscale >> {log} 2>&1;
    """

# -----------------------------------------------------------------------------#
#  STAGE 1: Nextclade QC
# -----------------------------------------------------------------------------#

rule_name = "nextclade_dataset"
rule nextclade_dataset:
  """Download Nextclade dataset."""

  message: """Downloading Nextclade dataset.\n
  log:     {log}
  dataset: {output.dataset_dir}
  """

  wildcard_constraints:
    # The tag will always begin with the year (ex. 2022)
    tag         = "([0-9]){4}.*",
  output:
    dataset_dir = directory("data/{dataset}_{tag}"),
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{dataset}}_{{tag}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{dataset}}_{{tag}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    nextclade dataset get --name {wildcards.dataset} --tag {wildcards.tag} --output-dir {output.dataset_dir} > {log} 2>&1;
    """

# -----------------------------------------------------------------------------
rule_name = "nextclade"

def _inputs_nextclade(build, prefix):
  inputs = {}

  if prefix == "nextclade_no-recomb":
    dataset = config["builds"][build]["nextclade_dataset"]["dataset_no-recomb"]
    tag = config["builds"][build]["nextclade_dataset"]["tag_no-recomb"]
  else:
    dataset = config["builds"][build]["nextclade_dataset"]["dataset"]
    tag = config["builds"][build]["nextclade_dataset"]["tag"]

  inputs["tag"] = tag
  inputs["dataset"] = dataset

  return inputs

rule nextclade:
  """Align sequences and perform QC with Nextclade."""

  message: """Aligning sequences and performing QC with Nextclade.\n
  build:     {wildcards.build}
  log:       {log}
  qc:        {output.qc}
  metadata:  {output.metadata}
  alignment: {output.alignment}
  """

  wildcard_constraints:
    nextclade_prefix = "nextclade|nextclade_no-recomb"
  input:
    dataset       = lambda wildcards:  "data/{dataset}_{tag}".format(
                      dataset=_inputs_nextclade(wildcards.build, wildcards.nextclade_prefix)["dataset"],
                      tag=_inputs_nextclade(wildcards.build, wildcards.nextclade_prefix)["tag"],
                      ),
    sequences     = "data/{build}/sequences.fasta",
    metadata      = "data/{build}/metadata.tsv",
  output:
    alignment     = "results/{build}/{nextclade_prefix}/alignment.fasta",
    qc            = "results/{build}/{nextclade_prefix}/qc.tsv",
    metadata      = "results/{build}/{nextclade_prefix}/metadata.tsv",
  params:
    outdir        = "results/{build}/{nextclade_prefix}",
    basename      = "nextclade",
    selection     = "fasta,tsv"
  benchmark:
    "benchmarks/{{nextclade_prefix}}/{{build}}_{today}.tsv".format(today=today),
  log:
    "logs/{{nextclade_prefix}}/{{build}}_{today}.log".format(today=today),
  shell:
    """
    # Align sequences
    nextclade run \
      --jobs {resources.cpus} \
      --input-dataset {input.dataset} \
      --output-all {params.outdir} \
      --output-selection {params.selection} \
      --output-tsv {output.qc} \
      --output-fasta {output.alignment} \
      --output-basename {params.basename} \
      {input.sequences} \
      >> {log} 2>&1;

    # Merge QC output with metadata
    csvtk rename -t -f "seqName" -n "strain" {output.qc} 2>> {log} \
      | csvtk merge -t -f "strain" {input.metadata} - \
      1> {output.metadata} 2>> {log};
    """

# ----------------------------------------------------------------------------#
rule_name = "nextclade_recombinants"

# Parameters function
def _params_nextclade_recombinants(build):
  """Parse conditional parameters for rule nextclade_recombinants."""

  params = {}
  exclude_clades = config["builds"][build]["nextclade_recombinants"]["exclude_clades"]
  if exclude_clades: params["exclude_clades"] = ".*(" + "|".join(exclude_clades) + ").*"
  else: params["exclude_clades"] = "None"

  return params

rule nextclade_recombinants:
  """Extract recombinants from nextclade output."""

  message: """Extracting recombinant candidates from nextclade output.\n
  build:     {wildcards.build}
  log:       {log}
  alignment: {output.alignment}
  strains:   {output.strains}
  """

  input:
    alignment      = "results/{build}/nextclade/alignment.fasta",
    qc             = "results/{build}/nextclade/qc.tsv",
  output:
    alignment      = "results/{build}/nextclade/recombinants.fasta",
    strains        = "results/{build}/nextclade/recombinants.txt",
    qc             = "results/{build}/nextclade/recombinants.qc.tsv",
    non            = "results/{build}/nextclade/non-recombinants.qc.tsv",
  params:
    exclude_clades = lambda wildcards: _params_nextclade_recombinants(wildcards.build)["exclude_clades"],
    fields         = "clade,Nextclade_pango,qc.overallStatus,privateNucMutations.labeledSubstitutions",
    values         = ".*(recombinant|X|bad|mediocre|\|).*", # \| is a char for the labelSubstitutions column
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Extract recombinant strains
    csvtk grep -t -v -f "clade" -r -p "{params.exclude_clades}" {input.qc}  2> {log} \
      | csvtk grep -t -f "qc.mixedSites.status" -p "bad" -v 2>> {log} \
      | csvtk grep -t -f "{params.fields}" -r -p "{params.values}" 2>> {log} \
      | csvtk cut -t -f "seqName" 2>> {log} \
      | tail -n+2 \
      > {output.strains};

    # Extract recombinant sequences
    seqkit grep -f {output.strains} {input.alignment} 1> {output.alignment} 2>> {log};

    # Extract recombinant qc
    csvtk grep -t -P {output.strains} {input.qc} 1> {output.qc} 2>> {log};

    # Extract non-recombinants qc
    csvtk grep -t -P {output.strains} -v {input.qc} 1> {output.non} 2>> {log};
    """

# -----------------------------------------------------------------------------#
#  STAGE 3: sc2rf
# -----------------------------------------------------------------------------#

rule_name = "sc2rf"

# Input function
def _inputs_sc2rf(build):
  """Parse conditional inputs for rule sc2rf."""

  inputs = {}
  exclude_negatives = config["builds"][build]["sc2rf"]["exclude_negatives"]

  # nextclade/alignment.fasta: positives, negatives, and false_positives
  # nextclade/recombinants.fasta: positives (and false_positives)

  # Option 1: Everything (positives, negatives, false_positives)
  if not exclude_negatives:
    inputs["alignment"] = "results/{build}/nextclade/alignment.fasta".format(build=build)

  # Option 2: Positives only
  else:
    inputs["alignment"] = "results/{build}/nextclade/recombinants.fasta".format(build=build)

  return inputs


# Snakemake rule
rule sc2rf:
  """
  Identify recombinants with sc2rf.
  """

  message: """Identifying recombinants with sc2rf.\n
  build:       {wildcards.build}
  log:         {log.primary}
  ansi:        {output.ansi}
  stats:       {output.csv}
  """

  input:
    alignment            = lambda wildcards: _inputs_sc2rf(wildcards.build)["alignment"],
  output:
    ansi                 = "results/{build}/sc2rf/ansi.txt",
    csv                  = "results/{build}/sc2rf/stats.csv",
    ansi_secondary       = "results/{build}/sc2rf/ansi.secondary.txt",
    csv_secondary        = "results/{build}/sc2rf/stats.secondary.csv",
  params:
    outdir               = "results/{build}",
    clades               = lambda wildcards: " ".join(config["builds"][wildcards.build]["sc2rf"]["clades"]),
    secondary_clades     = lambda wildcards: " ".join(config["builds"][wildcards.build]["sc2rf"]["secondary_clades"]),
    mutation_threshold   = lambda wildcards: config["builds"][wildcards.build]["sc2rf"]["mutation_threshold"],
    sc2rf_args           = lambda wildcards: config["builds"][wildcards.build]["sc2rf"]["sc2rf_args"],
    max_name_length      = lambda wildcards: config["builds"][wildcards.build]["sc2rf"]["max_name_length"]
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    primary   = "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
    secondary = "logs/{rule}/{{build}}_secondary_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Primary Mode
    bash scripts/sc2rf.sh \
      --alignment {input.alignment} \
      --output-ansi {output.ansi} \
      --output-csv {output.csv} \
      --log {log.primary} \
      --max-name-length {params.max_name_length} \
      --clades "{params.clades}" \
      --mutation-threshold {params.mutation_threshold} \
      {params.sc2rf_args};

    # Secondary Mode
    bash scripts/sc2rf.sh \
      --alignment {input.alignment} \
      --output-ansi {output.ansi_secondary} \
      --output-csv {output.csv_secondary} \
      --log {log.secondary} \
      --max-name-length {params.max_name_length} \
      --clades "{params.secondary_clades}" \
      --mutation-threshold {params.mutation_threshold} \
      {params.sc2rf_args};
    """

# -----------------------------------------------------------------------------
rule_name = "sc2rf_recombinants"

# Parameters function
def _params_sc2rf_recombinants(build):
  """Parse conditional parameters for rule sc2rf_recombinants."""

  params = {}
  motifs = config["builds"][build]["sc2rf_recombinants"]["motifs"]
  if motifs: params["motifs"] = "--motifs {}".format(motifs)
  else: params["motifs"] = ""

  min_len = config["builds"][build]["sc2rf_recombinants"]["min_len"]
  if min_len: params["min_len"] = "--min-len {}".format(min_len)
  else: params["min_len"] = ""

  max_breakpoints = config["builds"][build]["sc2rf_recombinants"]["max_breakpoints"]
  if max_breakpoints: params["max_breakpoints"] = "--max-breakpoints {}".format(max_breakpoints)
  else: params["max_breakpoints"] = ""

  max_parents = config["builds"][build]["sc2rf_recombinants"]["max_parents"]
  if max_parents: params["max_parents"] = "--max-parents {}".format(max_parents)
  else: params["max_parents"] = ""

  lapis = config["builds"][build]["sc2rf_recombinants"]["lapis"]
  nextclade_qc = "results/{build}/nextclade_no-recomb/qc.tsv".format(build=build)
  if lapis: params["lapis"] = "--nextclade {}".format(nextclade_qc)
  else: params["lapis"] = ""

  return params


# Snakemake rule
rule sc2rf_recombinants:
  """Postprocessing the sc2rf output to identify recombinants."""

  message: """Postprocessing the sc2rf output to identify recombinants.\n
  build:       {wildcards.build}
  log:         {log}
  stats:       {output.stats}
  ansi:        {output.ansi}
  alignment:   {output.alignment}
  """

  input:
    ansi            = rules.sc2rf.output.ansi,
    csv             = rules.sc2rf.output.csv,
    #ansi_secondary  = rules.sc2rf.output.ansi_secondary,
    csv_secondary   = rules.sc2rf.output.csv_secondary,
    issues          = rules.issues_download.output.issues,
    alignment       = lambda wildcards: _inputs_sc2rf(wildcards.build)["alignment"],
    nextclade       = "results/{build}/nextclade_no-recomb/qc.tsv",
  output:
    stats           = "results/{build}/sc2rf/stats.tsv",
    strains         = "results/{build}/sc2rf/recombinants.txt",
    alignment       = "results/{build}/sc2rf/recombinants.fasta",
    ansi            = "results/{build}/sc2rf/recombinants.ansi.txt",
  params:
    outdir          = "results/{build}/sc2rf",
    prefix          = "recombinants",
    min_len         = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["min_len"],
    max_parents     = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["max_parents"],
    max_breakpoints = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["max_breakpoints"],
    motifs          = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["motifs"],
    lapis           = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["lapis"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 sc2rf/postprocess.py \
      --csv {input.csv} \
      --csv-secondary {input.csv_secondary} \
      --ansi {input.ansi} \
      --prefix {params.prefix} \
      --outdir {params.outdir} \
      --aligned {input.alignment} \
      --issues {input.issues} \
      --log {log} \
      {params.motifs} \
      {params.min_len} \
      {params.max_parents} \
      {params.max_breakpoints} \
      {params.lapis} \
      >> {log} 2>&1;

    # Cleanup
    mv -f {params.outdir}/recombinants.tsv {params.outdir}/stats.tsv >> {log} 2>&1;
    rm -f {params.outdir}/recombinants.exclude.tsv >> {log} 2>&1;
    """

# -----------------------------------------------------------------------------

rule_name = "nextclade_exclude_false_positives"
rule nextclade_exclude_false_positives:
  """Exclude false positives from the nextclade alignment."""

  message: """Excluding false positives from the nextclade alignment.\n
  build:       {wildcards.build}
  log:         {log}
  alignment:   {output.alignment}
  """

  input:
    alignment = "results/{build}/nextclade/alignment.fasta",
    stats     = rules.sc2rf_recombinants.output.stats,
  output:
    alignment = "results/{build}/nextclade/alignment.exclude_false_positives.fasta",
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    csvtk grep -t -f "sc2rf_status" -p "false_positive" -v {input.stats} 2> {log} \
      | csvtk cut -t -f "strain" \
      | tail -n+2  \
      | seqkit grep -f - {input.alignment} 1> {output.alignment} 2>> {log} ;
    """

# -----------------------------------------------------------------------------#
#  STAGE 3: UShER
# -----------------------------------------------------------------------------#

rule_name = "faToVcf"

# Input function
def _inputs_faToVcf(build):
  """Parse conditional inputs for rule faToVcf."""

  inputs = {}
  exclude_negatives = config["builds"][build]["sc2rf"]["exclude_negatives"]
  exclude_false_positives = config["builds"][build]["faToVcf"]["exclude_false_positives"]

  # nextclade/alignment.fasta: positives, negatives, and false_positives
  # nextclade/recombinants.fasta: positives and false_positives
  # sc2rf/recombinants.fasta: positives

  # Option 1: Everything (positives, negatives, false_positives)
  # nextclade/alignment.fasta
  if not exclude_false_positives and not exclude_negatives:
    inputs["alignment"] = "results/{build}/nextclade/alignment.fasta".format(build=build)

  # Option 2: Positives and false_positives
  elif not exclude_false_positives and exclude_negatives:
    inputs["alignment"] = "results/{build}/nextclade/recombinants.fasta".format(build=build)

  # Option 3: Positives and negatives (no false_positives)
  elif exclude_false_positives and not exclude_negatives:
    inputs["alignment"] = "results/{build}/nextclade/alignment.exclude_false_positives.fasta".format(build=build)

  # Option 4: Positives only
  elif exclude_false_positives and exclude_negatives:
    inputs["alignment"] = "results/{build}/sc2rf/recombinants.fasta".format(build=build)

  # I think that covers it?
  else:
    print("ERROR: Unhandled exception in _inputs_faToVcf")
    quit()

  return inputs

# Snakemake rule
rule faToVcf:
  """
  Construct VCF from alignment.
  """
  message: """Constructing VCF from alignment.\n
  build: {wildcards.build}
  log:   {log}
  vcf:   {output.vcf}
  """

  input:
    reference               = "data/reference/reference.fasta",
    stats                   = rules.sc2rf_recombinants.output.stats,
    alignment               = lambda wildcards: _inputs_faToVcf(wildcards.build)["alignment"],
    prob_sites              = "data/reference/problematic_sites.vcf",
  output:
    alignment               = "results/{build}/faToVcf/alignment.fasta",
    vcf                     = "results/{build}/faToVcf/alignment.vcf.gz",
  params:
    prefix                  = "results/{build}/faToVcf/alignment.vcf",
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Add the reference genome
    cat {input.reference} {input.alignment} > {output.alignment} 2> {log};

    # Construct VCF
    faToVcf -ambiguousToN -maskSites={input.prob_sites} {output.alignment} {params.prefix} >> {log} 2>&1;

    # Compress Output
    gzip -f {params.prefix} >> {log} 2>&1;
    """

# -----------------------------------------------------------------------------
rule_name = "usher_download"

rule usher_download:
  """
  Download UShER protobuf.
  """

  message: """Downloading UShER protobuf.\n
  input:    {wildcards.input}
  log:      {log}
  protobuf: {output.pb}
  metadata: {output.metadata}
  """

  wildcard_constraints:
    input = "|".join(USHER_INPUTS),
  output:
    pb           = "data/{input}/usher.pb.gz",
    metadata     = "data/{input}/metadata.tsv.gz",
    ver          = "data/{input}/version.txt",
  params:
    pb_url       = lambda wildcards: config["inputs"][wildcards.input]["pb_url"],
    metadata_url = lambda wildcards: config["inputs"][wildcards.input]["metadata_url"],
    ver_url      = lambda wildcards: config["inputs"][wildcards.input]["ver_url"],
  benchmark:
    "benchmarks/{rule}/{{input}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{input}}_{today}.log".format(today=today, rule=rule_name),
  threads: 1
  resources:
    cpus = 1,
  shell:
    """
    wget -O {output.pb} {params.pb_url} > {log} 2>&1;
    wget -O {output.metadata} {params.metadata_url} >> {log} 2>&1;
    wget -O {output.ver} {params.ver_url} >> {log} 2>&1;
    """

# -----------------------------------------------------------------------------#
rule_name = "usher_columns"
rule usher_columns:
  """Add columns to the base phylogeny metadata."""

  message: """Adding extra columns to the UShER metadata.\n
  input:    {wildcards.input}
  log:      {log}
  metadata: {output.metadata}
  """

  wildcard_constraints:
    input = "|".join(USHER_INPUTS),
  input:
    metadata = "data/{input}/metadata.tsv.gz",
  output:
    metadata      = "data/{input}/metadata.tsv",
    gisaid_id_map = "data/{input}/gisaid_id_map.tsv",
  params:
    gisaid_regex = "^.*EPI_ISL_[0-9|-]*"
  benchmark:
    "benchmarks/{rule}/{{input}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{input}}_{today}.txt".format(today=today, rule=rule_name),
  threads: 1
  resources:
    cpus = 1,
    mem_mb = 4000,
  # Note: We use pipefail to allow `grep` to return a non-zero exit codes (1)
  #       which will happen if no GISAID IDs could be identified.
  shell:
    """
    set +o pipefail;

    # Extract strains with embedded GISID IDs
    csvtk cut -t -f "strain" {input.metadata} \
      | grep -o -E "{params.gisaid_regex}" \
      | awk -F "|" '{{print $0"\\t"$2}}' \
      > {output.gisaid_id_map};

    # Extract strains without GISAID IDs
    csvtk cut -t -f "strain" {input.metadata} \
      | tail -n+2 \
      | grep -v -E "{params.gisaid_regex}" \
      | awk '{{print $0"\\tNone"}}' \
      >> {output.gisaid_id_map};

    csvtk mutate2 -t -n "dataset" -e '"{wildcards.input}"' {input.metadata} \
      | csvtk rename -t -f "pangolin_lineage" -n "pango_lineage" \
      | csvtk mutate -t -f "strain" -n "gisaid_epi_isl" \
      | csvtk replace -t -f "gisaid_epi_isl" -p "(.*)" -k {output.gisaid_id_map} -r "{{kv}}" \
      1> {output.metadata} 2>> {log};
    """

# -----------------------------------------------------------------------------
rule_name = "usher"

# Input function
def _inputs_usher(build):
  """Parse conditional inputs for rule usher."""

  inputs = {}
  base_input = config["builds"][build]["base_input"]
  base_pb = "data/{base_input}/usher.pb.gz".format(base_input=base_input)
  inputs["base_input"] = base_input
  inputs["base_pb"] = base_pb
  return inputs

rule usher:
  """
  Place VCF samples with UShER.
  """

  message: """Placing VCF samples with UShER.\n
  build:    {wildcards.build}
  log:      {log}
  protobuf: {output.pb}
  """

  input:
    vcf             = rules.faToVcf.output.vcf,
    pb              = lambda wildcards: _inputs_usher(wildcards.build)["base_pb"],
  output:
    pb              = "results/{build}/usher/usher.pb.gz",
    final_tree      = "results/{build}/usher/final-tree.nh",
    placement_stats = "results/{build}/usher/placement_stats.tsv",
    clades          = "results/{build}/usher/clades.txt",
    mut_paths       = "results/{build}/usher/mutation-paths.txt",
  params:
    outdir = "results/{build}/usher",
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    usher \
      -i {input.pb} \
      -o {output.pb} \
      -v {input.vcf} \
      --threads {resources.cpus} \
      --outdir {params.outdir} \
      > {log} 2>&1;
    """

# -----------------------------------------------------------------------------
rule_name = "usher_stats"
rule usher_stats:
  """
  Parse statistics from UShER.
  """
  message: """Parsing statistics from UShER.\n
  build:      {wildcards.build}
  log:        {log}
  placements: {output.placement_stats}
  mutations:  {output.mut_paths}
  clades:     {output.clades}
  """

  input:
    placement_stats  = rules.usher.output.placement_stats,
    clades           = rules.usher.output.clades,
    mut_paths        = rules.usher.output.mut_paths,
    issue_to_lineage = rules.issues_download.output.issue_to_lineage,
  output:
    placement_stats  = "results/{build}/usher/usher.placement_stats.tsv",
    mut_paths        = "results/{build}/usher/usher.mutation_paths.tsv",
    clades           = "results/{build}/usher/usher.clades.tsv",
    strains          = "results/{build}/usher/usher.strains.txt",
  params:
    outdir = "results/{build}/usher",
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Rename and reformat output
    cp -f {input.mut_paths} {output.mut_paths};

    echo -e "strain\\tusher_best_set_difference\\tusher_num_best" > {output.placement_stats} 2> {log};
    sed 's/[[:space:]]*$//' {input.placement_stats} >> {output.placement_stats} 2>> {log};

    echo -e "strain\\tusher_clade\\tusher_pango_lineage" > {output.clades};

    # Some UShER trees may have 3 or 4 columns
    num_clade_cols=$(head -n 1 {input.clades} | tr "\\t" "\\n" | wc -l)
    if [[ $num_clade_cols -eq 4 ]]; then
      csvtk cut -t -H -f 1,2,4 {input.clades} >> {output.clades} 2>> {log};
    else
      cat {input.clades} >> {output.clades} 2>> {log};
    fi

    csvtk mutate -t -f "usher_pango_lineage" -n "usher_pango_lineage_map" {output.clades} 2>> {log} \
      | csvtk replace -t -f "usher_pango_lineage_map" -p "proposed([0-9]+)" -k {input.issue_to_lineage} -r "{{kv}}" 2>> {log} \
      1> {output.clades}.tmp 2>> {log};
    mv {output.clades}.tmp {output.clades} 2>> {log}

    # Strain List
    csvtk cut -t -f "strain" {output.clades}  2>> {log} | tail -n+2 > {output.strains};
    """

# -----------------------------------------------------------------------------#
#  STAGE 4: Summarize and sReport
# -----------------------------------------------------------------------------#

rule_name = "summary"

# Parameters function
def _params_summary(build):
  """Parse parameters from wildcards for rule summary."""

  params = {}

  extra_cols = config["builds"][build]["summary"]["extra_cols"]
  if extra_cols: params["extra_cols"] = "--extra-cols {}".format(",".join(extra_cols))
  else: params["extra_cols"] = ""

  return params

# Snakemake rule
rule summary:
  """Summarize results from pipeline tools."""

  message: """Summarizing results from pipeline tools.\n
  build:      {wildcards.build}
  log:        {log}
  summary:    {output.summary}
  """

  input:
    nextclade        = "results/{build}/nextclade/metadata.tsv",
    sc2rf            = rules.sc2rf_recombinants.output.stats,
    usher_clades     = rules.usher_stats.output.clades,
    usher_placements = rules.usher_stats.output.placement_stats,
    usher_ver        = lambda wildcards: "data/{base_input}/version.txt".format(
                        base_input=config["builds"][wildcards.build]["base_input"]
                      ),

  output:
    summary = "results/{build}/linelists/summary.tsv",
  params:
    extra_cols         = lambda wildcards: _params_summary(wildcards.build)["extra_cols"],
    nextclade_dataset  = lambda wildcards: config["builds"][wildcards.build]["nextclade_dataset"]["dataset"],
    nextclade_tag      = lambda wildcards: config["builds"][wildcards.build]["nextclade_dataset"]["tag"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    bash scripts/summary.sh \
      --output {output.summary} \
      --nextclade {input.nextclade} \
      --sc2rf {input.sc2rf} \
      --usher-clades {input.usher_clades} \
      --usher-placements {input.usher_placements} \
      --usher-dataset {input.usher_ver} \
      --nextclade-dataset {params.nextclade_dataset}_{params.nextclade_tag} \
      {params.extra_cols} \
      > {log} 2>&1;
    """

# -----------------------------------------------------------------------------#
# Linelist

# Parameters Function
def _params_linelist(build):
  """Parse parameters from wildcards for rule linelist."""

  params = {}

  geo = config["builds"][build]["linelist"]["geo"]
  if geo: params["geo"] = "--geo {}".format(geo)
  else: params["geo"] = ""

  max_placements = config["builds"][build]["linelist"]["max_placements"]
  if max_placements: params["max_placements"] = "--max-placements {}".format(max_placements)
  else: params["max_placements"] = ""

  return params


rule_name = "linelist"
rule linelist:
  """Create linelists of recombinant sequences, lineages, and parents."""

  message: """Creating linelists of recombinant sequences, lineages, and parents.\n
  build:             {wildcards.build}
  log:               {log}
  positives:         {output.positives}
  negatives:         {output.negatives}
  false_positives:   {output.false_positives}
  lineages:          {output.lineages}
  parents:           {output.parents}
  """

  input:
    summary         = rules.summary.output.summary,
    issues          = rules.issues_download.output.issues,
  output:
    linelist        = "results/{build}/linelists/linelist.tsv",
    positives       = "results/{build}/linelists/positives.tsv",
    negatives       = "results/{build}/linelists/negatives.tsv",
    false_positives = "results/{build}/linelists/false_positives.tsv",
    lineages        = "results/{build}/linelists/lineages.tsv",
    parents         = "results/{build}/linelists/parents.tsv",
  params:
    outdir         = "results/{build}/linelists",
    extra_cols     = lambda wildcards: _params_summary(wildcards.build)["extra_cols"],
    geo            = lambda wildcards: _params_linelist(wildcards.build)["geo"],
    max_placements = lambda wildcards: _params_linelist(wildcards.build)["max_placements"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Create the strain tables
    python3 scripts/linelist.py --input {input.summary} --issues {input.issues} --outdir {params.outdir} {params.max_placements} {params.extra_cols} > {log} 2>&1;

    # Create the lineages table
    python3 scripts/lineages.py --input {output.positives} --output {output.lineages} {params.geo} >> {log} 2>&1;

    # Create the parents table
    python3 scripts/parents.py --input {output.positives} --output {output.parents} >> {log} 2>&1;
    """

# -----------------------------------------------------------------------------#
# Plot

rule_name = "plot"

# Parameters Function
def _params_plot(build):
  """Parse parameters from wildcards for rule plot."""

  params = {}

  weeks = config["builds"][build]["plot"]["weeks"]
  if weeks: params["weeks"] = "--weeks {}".format(weeks)
  else: params["weeks"] = ""

  min_date = config["builds"][build]["plot"]["min_date"]
  if min_date: params["min_date"] = "--min-date {}".format(min_date)
  else: params["min_date"] = ""

  max_date = config["builds"][build]["plot"]["max_date"]
  if max_date: params["max_date"] = "--max-date {}".format(max_date)
  else: params["max_date"] = ""

  singletons = config["builds"][build]["plot"]["singletons"]
  if singletons: params["singletons"] = "--singletons"
  else: params["singletons"] = ""

  lag = config["builds"][build]["plot"]["lag"]
  if lag: params["lag"] = "--lag {}".format(lag)
  else: params["lag"] = ""

  return params

# Snakemake Rule
rule plot:
  """Plot results."""

  message: """Plotting results.\n
  build:   {wildcards.build}
  log:     {log}
  plots:   {output.plots}
  """

  input:
    positives  = rules.linelist.output.positives,
    lineages   = rules.linelist.output.lineages,
  output:
    plots      = directory("results/{build}/plots"),
  params:
    outdir     = "results/{build}/plots",
    geo        = lambda wildcards: _params_linelist(wildcards.build)["geo"],
    weeks      = lambda wildcards: _params_plot(wildcards.build)["weeks"],
    min_date   = lambda wildcards: _params_plot(wildcards.build)["min_date"],
    max_date   = lambda wildcards: _params_plot(wildcards.build)["max_date"],
    lag        = lambda wildcards: _params_plot(wildcards.build)["lag"],
    singletons = lambda wildcards: _params_plot(wildcards.build)["singletons"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 scripts/plot.py \
      --input {input.positives} \
      --outdir {params.outdir} \
      {params.lag} \
      {params.geo} \
      {params.weeks} \
      {params.min_date} \
      {params.max_date} \
      {params.singletons} \
      > {log} 2>&1;

    # Extract the cluster IDs to be plotted
    cluster_ids=$(csvtk headers -t {output.plots}/cluster_id.tsv | tail -n+2 | tr "\\n" "," | sed 's/,$/\\n/g')

    # Plot breakpoints by clade
    python3 scripts/plot_breakpoints.py \
      --lineages {input.lineages} \
      --positives {input.positives} \
      --outdir {params.outdir} \
      --parent-col parents_clade \
      --parent-type clade \
      --cluster-col cluster_id \
      --clusters "${{cluster_ids}}" \
      >> {log} 2>&1;

    # Plot breakpoints by lineage
    python3 scripts/plot_breakpoints.py \
      --lineages {input.lineages} \
      --positives {input.positives} \
      --outdir {params.outdir} \
      --parent-col parents_lineage \
      --parent-type lineage \
      --cluster-col cluster_id \
      --clusters "${{cluster_ids}}" \
      >> {log} 2>&1;
    """

# -----------------------------------------------------------------------------#
rule_name = "plot_historical"
rule plot_historical:
  """Plot results as a historical timeline."""

  message: """Plot results as a historical timeline.\n
  build:   {wildcards.build}
  log:     {log}
  plots:   {output.plots}
  """

  input:
    positives  = rules.linelist.output.positives,
    lineages   = rules.linelist.output.lineages,
  output:
    plots      = directory("results/{build}/plots_historical"),
  params:
    outdir     = "results/{build}/plots_historical",
    geo        = lambda wildcards: _params_linelist(wildcards.build)["geo"],
    lag        = lambda wildcards: _params_plot(wildcards.build)["lag"],
    singletons = lambda wildcards: _params_plot(wildcards.build)["singletons"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 scripts/plot.py \
      --input {input.positives} \
      --outdir {params.outdir} \
      {params.lag} \
      {params.geo} \
      {params.singletons} \
      > {log} 2>&1;

    # Plot breakpoints by clade
    python3 scripts/plot_breakpoints.py \
      --lineages {input.lineages} \
      --positives {input.positives} \
      --outdir {params.outdir} \
      --parent-col parents_clade \
      --parent-type clade \
      --cluster-col cluster_id \
      --autoscale \
      >> {log} 2>&1;

    # Plot breakpoints by lineage
    python3 scripts/plot_breakpoints.py \
      --lineages {input.lineages} \
      --positives {input.positives} \
      --outdir {params.outdir} \
      --parent-col parents_lineage \
      --parent-type lineage \
      --cluster-col cluster_id \
      --autoscale \
      >> {log} 2>&1;
    """

# -----------------------------------------------------------------------------#
# Report
# -----------------------------------------------------------------------------#

rule_name = "report"

def _params_report(build):
  """Parse parameters from wildcards for rule report."""

  params = {}

  geo = config["builds"][build]["linelist"]["geo"]
  if geo: params["geo"] = "--geo {}".format(geo)
  else: params["geo"] = ""

  template = config["builds"][build]["report"]["template"]
  if template: params["template"] = "--template {}".format(template)
  else: params["template"] = ""

  return params

rule report:
  """Summarize results into a report."""

  message: """Creating excel report and powerpoint slides.\n
  build:   {wildcards.build}
  log:     {log}
  report:  {output.xlsx}
  slides:  {output.pptx}
  """

  input:
    plots           = rules.plot.output.plots,
    tables          = [
                      rules.linelist.output.lineages,
                      rules.linelist.output.parents,
                      rules.linelist.output.linelist,
                      rules.linelist.output.positives,
                      rules.linelist.output.negatives,
                      rules.linelist.output.false_positives,
                      rules.summary.output.summary,
                      rules.issues_download.output.issues,
                      ],
  output:
    pptx            = "results/{build}/report/report.pptx",
    xlsx            = "results/{build}/report/report.xlsx",
  params:
    geo       = lambda wildcards: _params_report(wildcards.build)["geo"],
    template  = lambda wildcards: _params_report(wildcards.build)["template"],
    singletons = lambda wildcards: _params_plot(wildcards.build)["singletons"],

  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Create the excel report
    csvtk csv2xlsx -t -o {output.xlsx} {input.tables} > {log} 2>&1;

    # Create the powerpoint slides
    python3 scripts/report.py --plot-dir {input.plots} --output {output.pptx} {params.geo} {params.template} {params.singletons} >> {log} 2>&1;
    """

rule report_historical:
  """Summarize results into a historical report."""

  message: """Summarize results into a historical report.\n
  build:   {wildcards.build}
  log:     {log}
  slides:  {output.pptx}
  """

  input:
    plots           = rules.plot_historical.output.plots,
  output:
    pptx            = "results/{build}/report_historical/report.pptx",
  params:
    geo       = lambda wildcards: _params_report(wildcards.build)["geo"],
    template  = lambda wildcards: _params_report(wildcards.build)["template"],
    singletons = lambda wildcards: _params_plot(wildcards.build)["singletons"],

  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Create the powerpoint slides
    python3 scripts/report.py --plot-dir {input.plots} --output {output.pptx} {params.geo} {params.template} {params.singletons} > {log} 2>&1;
    """

# -----------------------------------------------------------------------------#
# Validation
# -----------------------------------------------------------------------------#

rule_name = "validate"

rule validate:
  """Validate recombinants of controlled datasets."""

  message: """Validating recombinants of controlled datasets.\n
  build:   {wildcards.build}
  log:     {log}
  table:   {output.table}
  """

  input:
    validation      = "defaults/validation.tsv",
    linelist        = "results/{build}/linelists/linelist.tsv",
  output:
    table           = "results/{build}/validate/validation.tsv",
  params:
    header          = "build\tpositives\texpected\tstatus",
    positive_status = "(designated|proposed|unpublished)",
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Compare the expected number of positives
    num_positives=$(csvtk grep -t -f "status" -r -p "{params.positive_status}"  {input.linelist} | tail -n+2 | wc -l)
    num_expected=$(csvtk grep -t -f "build" -p "{wildcards.build}" defaults/validation.tsv | csvtk cut -t -f "positives" | tail -n+2)

    if [[ $num_positives -eq $num_expected ]]; then
      build_status="PASS"
    else
      build_status="FAIL"
    fi

    # Create the output file
    echo "{params.header}" > {output.table};
    echo "{wildcards.build}\t${{num_positives}}\t${{num_expected}}\t${{build_status}}" >> {output.table}

    # Throw a pipeline error if it failed
    if [[ $build_status == "FAIL" ]]; then
      echo "Build failed validation: {wildcards.build}"
      echo
      csvtk csv2md -t {output.table}
      echo
      exit 1
    fi
    """
